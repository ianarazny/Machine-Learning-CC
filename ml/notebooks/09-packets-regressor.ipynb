{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Setup y Configuración\n",
    "# =========================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, List\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "\n",
    "# --- Parámetros principales ---\n",
    "TARGET_COL = \"packets\"        \n",
    "PREDICTION_MODE = \"next_step\"  # \"next_step\" (pronóstico paso siguiente) o \"final_total\" (tamaño final)\n",
    "PREFIX_LEN = 3                 # Para \"final_total\": cuántas filas iniciales por flujo usar\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Identidad de flujo (ajusta si usas otro identificador)\n",
    "FLOW_KEYS: List[str] = [\"src_ip\", \"dst_ip\", \"src_port\", \"dst_port\", \"protocol\"]\n",
    "\n",
    "# Columnas binarias (flags TCP)\n",
    "FLAG_COLS = [\"fin\",\"syn\",\"rst\",\"psh\",\"ack\",\"urg\"]\n",
    "\n",
    "# Nota técnica: El uso de ensambles tipo Random Forest para regresión en problemas de redes\n",
    "# está ampliamente documentado en la literatura de ML para networking (ver \"ml for networking.pdf\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# =========================\n",
    "# Carga de Datos\n",
    "# =========================\n",
    "# Reemplaza esta celda con tu origen real:\n",
    "#   - SQL: df = pd.read_sql(\"SELECT * FROM flow_metrics_logs ORDER BY ts ASC\", engine)\n",
    "#   - CSV: df = pd.read_csv(\"data/flow_metrics_logs.csv\")\n",
    "\n",
    "@dataclass\n",
    "class DBConfig:\n",
    "    host: str = os.getenv(\"DB_HOST\")\n",
    "    port: str = os.getenv(\"DB_PORT\")\n",
    "    db:   str = os.getenv(\"DB_NAME\")\n",
    "    user: str = os.getenv(\"DB_USER\")\n",
    "    pwd:  str = os.getenv(\"DB_PASSWORD\")\n",
    "    table: str = os.getenv(\"DATA_TABLE\", \"flow_metrics_logs\")  \n",
    "\n",
    "def make_engine(cfg: DBConfig):\n",
    "    url = f\"postgresql+psycopg2://{cfg.user}:{cfg.pwd}@{cfg.host}:{cfg.port}/{cfg.db}\"\n",
    "    return create_engine(url)\n",
    "\n",
    "cfg = DBConfig()\n",
    "engine = make_engine(cfg)\n",
    "engine\n",
    "\n",
    "Q = f\"\"\"\n",
    "SELECT\n",
    "  ts, pid, src_ip, dst_ip, src_port, dst_port, protocol,\n",
    "  flow_let, last_timestamp_ns, delta_ns, packets, bytes, \n",
    "  fin, syn, rst, psh, ack, urg ,throughput,\n",
    "  loss_est_pkts dup_acks_est\n",
    "FROM flow_metrics_logs\n",
    "ORDER BY ts ASC\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(Q, engine)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# =========================\n",
    "# Limpieza de Tipos y Valores\n",
    "# =========================\n",
    "numeric_like = [\n",
    "    \"delta_ns\", \"bytes\", \"throughput\", \"loss_est_pkts\",\n",
    "    \"dup_acks_est\", \"last_timestamp_ns\", TARGET_COL\n",
    "]\n",
    "\n",
    "for c in numeric_like:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "for c in FLAG_COLS:\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].fillna(0).astype(int)\n",
    "\n",
    "df = df.dropna(subset=[\"ts\"])  # asegurar timestamp\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Funciones: Objetivo y Utilidades\n",
    "# =========================\n",
    "def build_next_step_target(_df: pd.DataFrame, group_keys: List[str], target: str) -> pd.DataFrame:\n",
    "    g = _df.groupby(group_keys, sort=False)\n",
    "    out = _df.copy()\n",
    "    out[\"y\"] = g[target].shift(-1)            # valor acumulado en el próximo registro del flujo\n",
    "    out[\"lag_\"+target] = g[target].shift(1)   # lag como feature opcional\n",
    "    # eliminar el último punto de cada flujo (no tiene y)\n",
    "    last_mask = g.cumcount(ascending=True) == (g[target].transform(\"size\") - 1)\n",
    "    out = out[~last_mask].copy()\n",
    "    return out\n",
    "\n",
    "def build_final_total_target(_df: pd.DataFrame, group_keys: List[str], target: str, prefix_len: int) -> pd.DataFrame:\n",
    "    g = _df.groupby(group_keys, sort=False)\n",
    "    out = _df.copy()\n",
    "    out[\"y\"] = g[target].transform(\"last\")    # total acumulado final del flujo\n",
    "    out[\"rank_in_flow\"] = g.cumcount() + 1\n",
    "    out = out[out[\"rank_in_flow\"] <= prefix_len].copy()\n",
    "    return out\n",
    "\n",
    "def safe_rate(numer: pd.Series, denom_ns: pd.Series) -> pd.Series:\n",
    "    secs = denom_ns.astype(\"float64\") / 1e9\n",
    "    secs = secs.replace(0, np.nan)\n",
    "    return numer / secs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# =========================\n",
    "# Construcción del Dataset de Trabajo\n",
    "# =========================\n",
    "if PREDICTION_MODE == \"next_step\":\n",
    "    print(f'Total de muestras: {len(df)}')\t\n",
    "    work_df = build_next_step_target(df, FLOW_KEYS, TARGET_COL)\n",
    "    print(f\"Total muestras después de build_next_step_target: {len(work_df)}\")\n",
    "    print(work_df.head())\n",
    "elif PREDICTION_MODE == \"final_total\":\n",
    "    work_df = build_final_total_target(df, FLOW_KEYS, TARGET_COL, PREFIX_LEN)\n",
    "else:\n",
    "    raise ValueError(\"PREDICTION_MODE debe ser 'next_step' o 'final_total'.\")\n",
    "\n",
    "# Derivadas por flujo: difs y tasas\n",
    "g = work_df.groupby(FLOW_KEYS, sort=False)\n",
    "for col in [\"packets\", \"bytes\", \"last_timestamp_ns\"]:\n",
    "    if col in work_df.columns:\n",
    "        work_df[\"d_\"+col] = g[col].diff().fillna(0)\n",
    "\n",
    "if {\"d_packets\",\"delta_ns\"}.issubset(work_df.columns):\n",
    "    work_df[\"pps\"] = safe_rate(work_df[\"d_packets\"], work_df[\"delta_ns\"]).fillna(0)\n",
    "if {\"d_bytes\",\"delta_ns\"}.issubset(work_df.columns):\n",
    "    work_df[\"bps\"] = safe_rate(work_df[\"d_bytes\"], work_df[\"delta_ns\"]).fillna(0)\n",
    "\n",
    "work_df = work_df.dropna(subset=[\"y\"]).reset_index(drop=True)\n",
    "work_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# =========================\n",
    "# Selección de Features\n",
    "# =========================\n",
    "cat_features = [c for c in [\"protocol\"] if c in work_df.columns]  # categóricas a OHE\n",
    "\n",
    "num_features_base = [\n",
    "    \"delta_ns\",\"bytes\",\"throughput\",\"loss_est_pkts\",\"dup_acks_est\",\n",
    "    \"d_packets\",\"d_bytes\",\"pps\",\"bps\",\"src_port\",\"dst_port\",\"flow_let\"\n",
    "]\n",
    "num_features = [c for c in num_features_base if c in work_df.columns]\n",
    "\n",
    "# lag del objetivo como feature (solo en next_step)\n",
    "if PREDICTION_MODE == \"next_step\":\n",
    "    lag_col = \"lag_\"+TARGET_COL\n",
    "    if lag_col in work_df.columns:\n",
    "        num_features.append(lag_col)\n",
    "\n",
    "bin_features = [c for c in FLAG_COLS if c in work_df.columns]\n",
    "\n",
    "# No usar directamente el TARGET_COL ni columnas que filtramos\n",
    "drop_cols = set([TARGET_COL, \"y\", \"rank_in_flow\"])\n",
    "basic_cols = set(FLOW_KEYS + [\"ts\"])\n",
    "X_cols = [c for c in (num_features + bin_features + cat_features)\n",
    "          if c not in drop_cols and c not in basic_cols]\n",
    "\n",
    "# Quitar columnas completamente NaN\n",
    "X_cols = [c for c in X_cols if work_df[c].notna().any()]\n",
    "\n",
    "X_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# =========================\n",
    "# Pipeline (Pre + Modelo)\n",
    "# =========================\n",
    "numeric_cols = [c for c in X_cols if c not in [\"protocol\"]]\n",
    "categorical_cols = [c for c in X_cols if c in [\"protocol\"]]\n",
    "\n",
    "pre = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(with_mean=False), numeric_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True), categorical_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=300,\n",
    "    max_depth=None,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"pre\", pre),\n",
    "    (\"model\", model),\n",
    "])\n",
    "\n",
    "pipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# =========================\n",
    "# Validación Cruzada por Flujo\n",
    "# =========================\n",
    "y = work_df[\"y\"].astype(\"float64\").values\n",
    "groups = work_df.groupby(FLOW_KEYS, sort=False).ngroup().values\n",
    "\n",
    "gkf = GroupKFold(n_splits=N_SPLITS)\n",
    "fold_metrics: List[Dict] = []\n",
    "y_true_all, y_pred_all = [], []\n",
    "\n",
    "for fold, (tr, te) in enumerate(gkf.split(work_df[X_cols], y, groups=groups), 1):\n",
    "    X_tr = work_df.iloc[tr][X_cols]\n",
    "    X_te = work_df.iloc[te][X_cols]\n",
    "    y_tr = y[tr]\n",
    "    y_te = y[te]\n",
    "\n",
    "    pipe.fit(X_tr, y_tr)\n",
    "    pred = pipe.predict(X_te)\n",
    "\n",
    "    mae  = mean_absolute_error(y_te, pred)\n",
    "    rmse = mean_squared_error(y_te, pred, squared=False)\n",
    "    r2   = r2_score(y_te, pred)\n",
    "\n",
    "    fold_metrics.append({\"fold\": fold, \"MAE\": mae, \"RMSE\": rmse, \"R2\": r2})\n",
    "    y_true_all.append(y_te)\n",
    "    y_pred_all.append(pred)\n",
    "\n",
    "# Métricas globales\n",
    "y_true_all = np.concatenate(y_true_all) if len(y_true_all) else np.array([])\n",
    "y_pred_all = np.concatenate(y_pred_all) if len(y_pred_all) else np.array([])\n",
    "\n",
    "print(\"=== Resultados CV (GroupKFold por flujo) ===\")\n",
    "for m in fold_metrics:\n",
    "    print(f\"[FOLD {m['fold']}] MAE={m['MAE']:.4f}  RMSE={m['RMSE']:.4f}  R2={m['R2']:.4f}\")\n",
    "\n",
    "if y_true_all.size:\n",
    "    mae  = mean_absolute_error(y_true_all, y_pred_all)\n",
    "    rmse = mean_squared_error(y_true_all, y_pred_all, squared=False)\n",
    "    r2   = r2_score(y_true_all, y_pred_all)\n",
    "    print(f\"[GLOBAL] MAE={mae:.4f}  RMSE={rmse:.4f}  R2={r2:.4f}\")\n",
    "else:\n",
    "    print(\"No se acumularon predicciones; revisa que haya suficientes datos por flujo.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# =========================\n",
    "# Entrenamiento Final y Exportación\n",
    "# =========================\n",
    "pipe.fit(work_df[X_cols], work_df[\"y\"])\n",
    "\n",
    "artifact = {\n",
    "    \"pipeline\": pipe,\n",
    "    \"x_cols\": X_cols,\n",
    "    \"flow_keys\": FLOW_KEYS,\n",
    "    \"target\": TARGET_COL,\n",
    "    \"mode\": PREDICTION_MODE,\n",
    "    \"prefix_len\": PREFIX_LEN\n",
    "}\n",
    "\n",
    "joblib.dump(artifact, f\"regressor_flow_{TARGET_COL}.joblib\")\n",
    "print(f\"Modelo guardado en regressor_flow_{TARGET_COL}.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# =========================\n",
    "# Función para reentrenar con otro objetivo\n",
    "# =========================\n",
    "def train_flow_regressor(\n",
    "    df_in: pd.DataFrame,\n",
    "    target_col: str = \"packets\",\n",
    "    mode: str = \"next_step\",\n",
    "    prefix_len: int = 3,\n",
    "    n_splits: int = 5,\n",
    "    random_state: int = 42\n",
    ") -> Dict:\n",
    "    # 1) ordenar y typificar\n",
    "    df2 = df_in.sort_values(FLOW_KEYS + [\"ts\"]).reset_index(drop=True).copy()\n",
    "    for c in [\"delta_ns\",\"bytes\",\"throughput\",\"loss_est_pkts\",\"dup_acks_est\",\"last_timestamp_ns\",target_col]:\n",
    "        if c in df2.columns:\n",
    "            df2[c] = pd.to_numeric(df2[c], errors=\"coerce\")\n",
    "    for c in FLAG_COLS:\n",
    "        if c in df2.columns:\n",
    "            df2[c] = df2[c].fillna(0).astype(int)\n",
    "\n",
    "    # 2) objetivo\n",
    "    if mode == \"next_step\":\n",
    "        wk = build_next_step_target(df2, FLOW_KEYS, target_col)\n",
    "    elif mode == \"final_total\":\n",
    "        wk = build_final_total_target(df2, FLOW_KEYS, target_col, prefix_len)\n",
    "    else:\n",
    "        raise ValueError(\"mode debe ser 'next_step' o 'final_total'.\")\n",
    "\n",
    "    # 3) features\n",
    "    g = wk.groupby(FLOW_KEYS, sort=False)\n",
    "    for col in [\"packets\",\"bytes\",\"last_timestamp_ns\"]:\n",
    "        if col in wk.columns:\n",
    "            wk[\"d_\"+col] = g[col].diff().fillna(0)\n",
    "    if {\"d_packets\",\"delta_ns\"}.issubset(wk.columns):\n",
    "        wk[\"pps\"] = safe_rate(wk[\"d_packets\"], wk[\"delta_ns\"]).fillna(0)\n",
    "    if {\"d_bytes\",\"delta_ns\"}.issubset(wk.columns):\n",
    "        wk[\"bps\"] = safe_rate(wk[\"d_bytes\"], wk[\"delta_ns\"]).fillna(0)\n",
    "    wk = wk.dropna(subset=[\"y\"]).reset_index(drop=True)\n",
    "\n",
    "    cat_features = [c for c in [\"protocol\"] if c in wk.columns]\n",
    "    num_features = [\n",
    "        c for c in [\n",
    "            \"delta_ns\",\"bytes\",\"throughput\",\"loss_est_pkts\",\"dup_acks_est\",\n",
    "            \"d_packets\",\"d_bytes\",\"pps\",\"bps\",\"src_port\",\"dst_port\",\"flow_let\",\n",
    "            (\"lag_\"+target_col if mode == \"next_step\" else None)\n",
    "        ] if c and c in wk.columns\n",
    "    ]\n",
    "    bin_features = [c for c in FLAG_COLS if c in wk.columns]\n",
    "\n",
    "    drop_cols = set([target_col, \"y\", \"rank_in_flow\"])\n",
    "    basic_cols = set(FLOW_KEYS + [\"ts\"])\n",
    "    X_cols_local = [c for c in (num_features + bin_features + cat_features)\n",
    "                    if c not in drop_cols and c not in basic_cols and wk[c].notna().any()]\n",
    "\n",
    "    numeric_cols = [c for c in X_cols_local if c not in [\"protocol\"]]\n",
    "    categorical_cols = [c for c in X_cols_local if c in [\"protocol\"]]\n",
    "\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", StandardScaler(with_mean=False), numeric_cols),\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True), categorical_cols),\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=300, max_depth=None, random_state=random_state, n_jobs=-1\n",
    "    )\n",
    "    pipe = Pipeline([(\"pre\", pre), (\"model\", model)])\n",
    "\n",
    "    y_local = wk[\"y\"].astype(\"float64\").values\n",
    "    groups = wk.groupby(FLOW_KEYS, sort=False).ngroup().values\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "    fold_metrics, y_true_all, y_pred_all = [], [], []\n",
    "    for fold, (tr, te) in enumerate(gkf.split(wk[X_cols_local], y_local, groups=groups), 1):\n",
    "        pipe.fit(wk.iloc[tr][X_cols_local], y_local[tr])\n",
    "        pred = pipe.predict(wk.iloc[te][X_cols_local])\n",
    "        mae  = mean_absolute_error(y_local[te], pred)\n",
    "        rmse = mean_squared_error(y_local[te], pred, squared=False)\n",
    "        r2   = r2_score(y_local[te], pred)\n",
    "        fold_metrics.append({\"fold\": fold, \"MAE\": mae, \"RMSE\": rmse, \"R2\": r2})\n",
    "        y_true_all.append(y_local[te])\n",
    "        y_pred_all.append(pred)\n",
    "\n",
    "    if y_true_all:\n",
    "        y_true_all = np.concatenate(y_true_all)\n",
    "        y_pred_all = np.concatenate(y_pred_all)\n",
    "        mae  = mean_absolute_error(y_true_all, y_pred_all)\n",
    "        rmse = mean_squared_error(y_true_all, y_pred_all, squared=False)\n",
    "        r2   = r2_score(y_true_all, y_pred_all)\n",
    "    else:\n",
    "        mae = rmse = r2 = np.nan\n",
    "\n",
    "    # Entrenamiento final\n",
    "    pipe.fit(wk[X_cols_local], wk[\"y\"])\n",
    "    artifact = {\n",
    "        \"pipeline\": pipe,\n",
    "        \"x_cols\": X_cols_local,\n",
    "        \"flow_keys\": FLOW_KEYS,\n",
    "        \"target\": target_col,\n",
    "        \"mode\": mode,\n",
    "        \"prefix_len\": prefix_len,\n",
    "        \"cv_metrics\": fold_metrics,\n",
    "        \"global_metrics\": {\"MAE\": mae, \"RMSE\": rmse, \"R2\": r2},\n",
    "    }\n",
    "    return artifact\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Ejemplo: entrenar para \"bytes\"\n",
    "# =========================\n",
    "# artifact_bytes = train_flow_regressor(df, target_col=\"bytes\", mode=\"next_step\", prefix_len=3)\n",
    "# joblib.dump(artifact_bytes, \"regressor_flow_bytes.joblib\")\n",
    "# artifact_bytes[\"global_metrics\"]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
