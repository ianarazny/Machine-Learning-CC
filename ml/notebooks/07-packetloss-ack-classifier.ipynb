{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "505dfe5c",
   "metadata": {},
   "source": [
    "\n",
    "# Clasificador de Congestión con Packet-Loss y ACKs Duplicados\n",
    "\n",
    "**Objetivo:** Entrenar y evaluar modelos **Random Forest** y **XGBoost** que clasifiquen **congestión (1) / no congestión (0)** usando **solo dos features**:\n",
    "1. `packet_loss_rate`\n",
    "2. `dup_acks` (o un **proxy** si no está disponible directamente)\n",
    "\n",
    "> **Contexto académico (resumen):** La pérdida de paquetes y los ACKs duplicados son señales clásicas para inferir congestión. Distintas propuestas de ML para redes han usado supervisión con árboles (boosting/Random Forest) para distinguir pérdidas por congestión de otras causas y mejorar TCP, reportando mejoras significativas en detección y throughput. Ver, por ejemplo, *Boutaba et al., 2018 (ml for networking)* y notas del proyecto (*Proceso.pdf*).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78b9271",
   "metadata": {},
   "source": [
    "\n",
    "## Requisitos\n",
    "- Python 3.9+\n",
    "- `pandas`, `numpy`, `scikit-learn`, `xgboost`, `matplotlib`, `joblib`\n",
    "\n",
    "Instalación (opcional):\n",
    "\n",
    "```bash\n",
    "pip install pandas numpy scikit-learn xgboost matplotlib joblib\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fd4bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =====================\n",
    "# Configuración general\n",
    "# =====================\n",
    "CSV_PATH = \"../data/mi_dataset.csv\"  # <-- Cambia a tu ruta real\n",
    "TIMESTAMP_COL = \"ts\"                 # Si existe; si no, dejar None\n",
    "FLOW_KEYS = [\"src_ip\", \"dst_ip\", \"src_port\", \"dst_port\", \"protocol\"]  # opcional si existen\n",
    "\n",
    "# Umbrales (para fallback de etiquetas si no existe columna de target)\n",
    "TH_PACKET_LOSS = 0.01   # 1% de pérdida\n",
    "TH_DUP_ACKS = 3         # 3 ACKs duplicados en ventana\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "N_JOBS = -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884bfc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, Optional, List\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, roc_auc_score, roc_curve\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import dump\n",
    "\n",
    "# Asegurar opciones de Pandas legibles\n",
    "pd.set_option(\"display.max_columns\", 120)\n",
    "pd.set_option(\"display.width\", 160)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c1c6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_dataset(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Carga el CSV. Ajusta el separador, encoding y parseo de fechas según tu caso.\n",
    "    Este ejemplo asume separador ';' porque tu dataset de referencia lo usa así.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"No se encuentra el archivo: {path}\")\n",
    "    df = pd.read_csv(path, sep=';', engine='python')\n",
    "    # Si hay una columna basura como 'Unnamed: 19', la eliminamos:\n",
    "    cols_to_drop = [c for c in df.columns if c.lower().startswith(\"unnamed\")]\n",
    "    if cols_to_drop:\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bda6276",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def derive_features(\n",
    "    df: pd.DataFrame,\n",
    "    flow_keys: Optional[List[str]] = None,\n",
    "    timestamp_col: Optional[str] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Deriva dos features:\n",
    "    - packet_loss_rate: a partir de bytes_retrans / bytes_sent (si existen).\n",
    "    - dup_acks: si existe columna directa (p.ej. dup_acks), úsala; si no, arma un PROXY.\n",
    "\n",
    "    PROXY de dup_acks (sugerencia inicial, ajustable a tu dataset):\n",
    "    - Usar diferencias por flujo y tiempo entre 'bytes_acked' y 'data_segs_out' / 'segs_out'. \n",
    "      En TCP, varios ACKs por el mismo segmento pueden coincidir con ventanas deslizantes y/o\n",
    "      reordenamientos; aquí se construye un proxy conservador basado en picos en 'segs_in' \n",
    "      no acompañados de progreso en 'bytes_acked'. **Ajusta esta lógica cuando tengas las columnas definitivas.**\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # packet_loss_rate\n",
    "    if {\"bytes_retrans\", \"bytes_sent\"} <= set(df.columns):\n",
    "        denom = (df[\"bytes_sent\"].astype(float).abs() + 1e-9)\n",
    "        df[\"packet_loss_rate\"] = (df[\"bytes_retrans\"].astype(float).clip(lower=0)) / denom\n",
    "    else:\n",
    "        # Fallback: si no existen, crea NaN y avisa\n",
    "        df[\"packet_loss_rate\"] = np.nan\n",
    "        print(\"[WARN] No se encontraron columnas 'bytes_retrans' y/o 'bytes_sent'. packet_loss_rate = NaN.\")\n",
    "\n",
    "    # dup_acks (directa o proxy)\n",
    "    if \"dup_acks\" in df.columns:\n",
    "        df[\"dup_acks\"] = df[\"dup_acks\"].astype(float).clip(lower=0)\n",
    "    else:\n",
    "        # PROXY de dup_acks:\n",
    "        # 1) Ordenar por flujo y tiempo si están disponibles\n",
    "        if flow_keys and all(k in df.columns for k in flow_keys):\n",
    "            sort_cols = flow_keys.copy()\n",
    "        else:\n",
    "            sort_cols = []\n",
    "        if timestamp_col and timestamp_col in df.columns:\n",
    "            sort_cols.append(timestamp_col)\n",
    "        if sort_cols:\n",
    "            df = df.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "        # 2) Diferencias no negativas de segs_in y data_segs_out/ segs_out\n",
    "        segs_in = df[\"segs_in\"].astype(float) if \"segs_in\" in df.columns else pd.Series(0.0, index=df.index)\n",
    "        if \"data_segs_out\" in df.columns:\n",
    "            out = df[\"data_segs_out\"].astype(float)\n",
    "        elif \"segs_out\" in df.columns:\n",
    "            out = df[\"segs_out\"].astype(float)\n",
    "        else:\n",
    "            out = pd.Series(0.0, index=df.index)\n",
    "\n",
    "        d_in = segs_in.diff().fillna(0).clip(lower=0)\n",
    "        d_out = out.diff().fillna(0).clip(lower=0)\n",
    "\n",
    "        # 3) Diferencias en bytes_acked\n",
    "        if \"bytes_acked\" in df.columns:\n",
    "            d_bytes_acked = df[\"bytes_acked\"].astype(float).diff().fillna(0).clip(lower=0)\n",
    "        else:\n",
    "            d_bytes_acked = pd.Series(0.0, index=df.index)\n",
    "\n",
    "        # 4) Proxy: picos de llegadas (ACKs recibidos) sin progreso proporcional en bytes_acked\n",
    "        #    Fórmula simple: exceso = d_in - (d_bytes_acked / MSS_aprox_en_segmentos)\n",
    "        #    Si no hay mss, usamos un MSS aproximado de 1460 bytes para convertir bytes a \"segmentos\" aprox.\n",
    "        MSS_APROX = 1460.0\n",
    "        proxy = d_in - (d_bytes_acked / MSS_APROX)\n",
    "        proxy = (proxy + (d_in - d_out) * 0.0).clip(lower=0)  # mantener no negativo\n",
    "\n",
    "        # Suavizado opcional: ventana pequeña\n",
    "        df[\"dup_acks\"] = proxy.rolling(window=3, min_periods=1).mean()\n",
    "\n",
    "    # Mantener solo features finales + columnas útiles para referencia\n",
    "    keep = [\"packet_loss_rate\", \"dup_acks\"]\n",
    "    extras = [c for c in [timestamp_col] if c and c in df.columns]\n",
    "    df_feats = df[keep + extras].copy()\n",
    "    return df_feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763dbe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_labels(\n",
    "    df_original: pd.DataFrame,\n",
    "    df_feats: pd.DataFrame,\n",
    "    th_packet_loss: float = 0.01,\n",
    "    th_dup_acks: float = 3.0,\n",
    ") -> pd.Series:\n",
    "    \"\"\"Devuelve la etiqueta binaria 'congestion'.\n",
    "    - Si df_original tiene una columna 'congestion', se usa esa.\n",
    "    - Si no, se usa una **regla de fallback** con umbrales:\n",
    "      congestión = 1 si packet_loss_rate >= th_packet_loss o dup_acks >= th_dup_acks\n",
    "    \"\"\"\n",
    "    if \"congestion\" in df_original.columns:\n",
    "        y = df_original[\"congestion\"].astype(int)\n",
    "    else:\n",
    "        pl = df_feats[\"packet_loss_rate\"].fillna(0)\n",
    "        da = df_feats[\"dup_acks\"].fillna(0)\n",
    "        y = ((pl >= th_packet_loss) | (da >= th_dup_acks)).astype(int)\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43289f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_models(X: pd.DataFrame, y: pd.Series):\n",
    "    # Random Forest\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=None,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=N_JOBS,\n",
    "        class_weight=None\n",
    "    )\n",
    "    rf.fit(X, y)\n",
    "\n",
    "    # XGBoost\n",
    "    xgb = XGBClassifier(\n",
    "        n_estimators=500,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        min_child_weight=1.0,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=N_JOBS,\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"logloss\",\n",
    "        tree_method=\"hist\"\n",
    "    )\n",
    "    xgb.fit(X, y)\n",
    "\n",
    "    return rf, xgb\n",
    "\n",
    "\n",
    "def evaluate_models(models, X_train, y_train, X_test, y_test):\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        y_pred = model.predict(X_test)\n",
    "        try:\n",
    "            y_prob = model.predict_proba(X_test)[:, 1]\n",
    "        except Exception:\n",
    "            # Algunos modelos podrían no tener predict_proba\n",
    "            y_prob = None\n",
    "\n",
    "        metrics = {\n",
    "            \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "            \"precision\": precision_score(y_test, y_pred, zero_division=0),\n",
    "            \"recall\": recall_score(y_test, y_pred, zero_division=0),\n",
    "            \"f1\": f1_score(y_test, y_pred, zero_division=0),\n",
    "        }\n",
    "        if y_prob is not None and len(np.unique(y_test)) == 2:\n",
    "            try:\n",
    "                metrics[\"roc_auc\"] = roc_auc_score(y_test, y_prob)\n",
    "            except Exception:\n",
    "                metrics[\"roc_auc\"] = np.nan\n",
    "        else:\n",
    "            metrics[\"roc_auc\"] = np.nan\n",
    "\n",
    "        results[name] = metrics\n",
    "    return results\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title=\"Matriz de confusión\"):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(4.5, 4.5))\n",
    "    im = ax.imshow(cm, interpolation='nearest')\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]))\n",
    "    ax.set_xlabel(\"Predicción\")\n",
    "    ax.set_ylabel(\"Real\")\n",
    "    ax.set_title(title)\n",
    "    # Etiquetas\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], 'd'), ha=\"center\", va=\"center\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_roc(y_true, y_prob, title=\"ROC Curve\"):\n",
    "    if y_prob is None:\n",
    "        print(\"[INFO] El modelo no provee probabilidades. ROC omitida.\")\n",
    "        return\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    fig, ax = plt.subplots(figsize=(5.5, 4.0))\n",
    "    ax.plot(fpr, tpr, label=\"ROC\")\n",
    "    ax.plot([0,1], [0,1], linestyle=\"--\")\n",
    "    ax.set_xlabel(\"FPR\")\n",
    "    ax.set_ylabel(\"TPR\")\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_feature_importance(model, feature_names, title):\n",
    "    if hasattr(model, \"feature_importances_\"):\n",
    "        importances = model.feature_importances_\n",
    "    elif hasattr(model, \"get_booster\"):\n",
    "        importances = model.get_booster().get_score(importance_type=\"gain\")\n",
    "        # Map a vector consistent with feature order if returned as dict\n",
    "        if isinstance(importances, dict):\n",
    "            # XGBoost names features as f0, f1, ...\n",
    "            vect = np.zeros(len(feature_names))\n",
    "            for i, _ in enumerate(feature_names):\n",
    "                vect[i] = importances.get(f\"f{i}\", 0.0)\n",
    "            importances = vect\n",
    "    else:\n",
    "        print(\"[INFO] El modelo no expone importancias de features.\")\n",
    "        return\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6.0, 4.0))\n",
    "    idx = np.argsort(importances)[::-1]\n",
    "    ax.bar(range(len(feature_names)), np.array(importances)[idx])\n",
    "    ax.set_xticks(range(len(feature_names)))\n",
    "    ax.set_xticklabels([feature_names[i] for i in idx], rotation=0)\n",
    "    ax.set_title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e42680a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =====================\n",
    "# Flujo principal\n",
    "# =====================\n",
    "df = load_dataset(CSV_PATH)\n",
    "print(\"Shape:\", df.shape)\n",
    "display(df.head(3))\n",
    "\n",
    "# Derivar features (solo 2 features)\n",
    "df_feats = derive_features(df, flow_keys=FLOW_KEYS, timestamp_col=TIMESTAMP_COL)\n",
    "print(\"Features derivadas:\")\n",
    "display(df_feats.head(5))\n",
    "\n",
    "# Etiquetas\n",
    "y = get_labels(df, df_feats, th_packet_loss=TH_PACKET_LOSS, th_dup_acks=TH_DUP_ACKS)\n",
    "print(\"Distribución de etiquetas (0=no congestión, 1=congestión):\")\n",
    "print(y.value_counts(dropna=False))\n",
    "\n",
    "# Dataset final para ML (solo 2 columnas)\n",
    "X = df_feats[[\"packet_loss_rate\", \"dup_acks\"]].copy().fillna(0.0)\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "# Entrenar\n",
    "rf, xgb = train_models(X_train, y_train)\n",
    "\n",
    "# Evaluar\n",
    "models = {\"RandomForest\": rf, \"XGBoost\": xgb}\n",
    "results = evaluate_models(models, X_train, y_train, X_test, y_test)\n",
    "print(\"Resultados en test:\")\n",
    "for name, m in results.items():\n",
    "    print(f\"{name}: \" + \", \".join([f\"{k}={v:.4f}\" if isinstance(v, (int, float)) else f\"{k}={v}\" for k,v in m.items()]))\n",
    "\n",
    "# Predicciones para gráficos\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "try:\n",
    "    y_prob_rf = rf.predict_proba(X_test)[:, 1]\n",
    "except Exception:\n",
    "    y_prob_rf = None\n",
    "\n",
    "y_pred_xgb = xgb.predict(X_test)\n",
    "try:\n",
    "    y_prob_xgb = xgb.predict_proba(X_test)[:, 1]\n",
    "except Exception:\n",
    "    y_prob_xgb = None\n",
    "\n",
    "# Gráficos (uno por plot)\n",
    "plot_confusion_matrix(y_test, y_pred_rf, title=\"Matriz de confusión - RandomForest\")\n",
    "plot_confusion_matrix(y_test, y_pred_xgb, title=\"Matriz de confusión - XGBoost\")\n",
    "\n",
    "plot_roc(y_test, y_prob_rf, title=\"ROC - RandomForest\")\n",
    "plot_roc(y_test, y_prob_xgb, title=\"ROC - XGBoost\")\n",
    "\n",
    "# Importancias\n",
    "plot_feature_importance(rf, X.columns.tolist(), title=\"Importancia de features - RandomForest\")\n",
    "plot_feature_importance(xgb, X.columns.tolist(), title=\"Importancia de features - XGBoost\")\n",
    "\n",
    "# Guardar modelos y columnas\n",
    "Path(\"/mnt/data\").mkdir(parents=True, exist_ok=True)\n",
    "dump(rf, \"/mnt/data/rf_congestion_classifier.joblib\")\n",
    "dump(xgb, \"/mnt/data/xgb_congestion_classifier.joblib\")\n",
    "pd.Series(X.columns, name=\"feature_names\").to_csv(\"/mnt/data/feature_names.csv\", index=False)\n",
    "\n",
    "print(\"\\nModelos guardados:\") \n",
    "print(\"- /mnt/data/rf_congestion_classifier.joblib\") \n",
    "print(\"- /mnt/data/xgb_congestion_classifier.joblib\") \n",
    "print(\"- /mnt/data/feature_names.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1bee21",
   "metadata": {},
   "source": [
    "\n",
    "## Próximos pasos\n",
    "1. **Ajustar `derive_features(...)`** para que `dup_acks` use tu columna real (si existe) o un proxy más fiel a tu logging.\n",
    "2. **Reemplazar el fallback de etiquetas** si ya cuentas con `congestion` (o un label derivado de tu ground truth).\n",
    "3. Incorporar validación temporal (train/test split por tiempo) si tus datos son series temporales.\n",
    "4. Registrar parámetros/artefactos con MLflow o similar para trazabilidad.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
